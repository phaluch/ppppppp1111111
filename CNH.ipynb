{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f855f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 200\n",
    "GOOD_MATCH_PERCENT = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignImages(im1, im2):\n",
    "    \n",
    "    #Convert to grayscale\n",
    "    im1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "    im2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect ORB features and compute descriptors\n",
    "    orb = cv2.ORB_create(MAX_FEATURES)\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(im1Gray, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(im2Gray, None)\n",
    "    \n",
    "    #Match features\n",
    "    matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "    matches = matcher.match(descriptors1, descriptors2, None)\n",
    "    \n",
    "    #Sort matches by score\n",
    "    matches.sort(key = lambda x: x.distance, reverse=True)\n",
    "    \n",
    "    # Remove not so good matches\n",
    "    numGoodMatches = int(len(matches)* GOOD_MATCH_PERCENT)\n",
    "    matches = matches[:numGoodMatches]\n",
    "    \n",
    "    # Draw top matches\n",
    "    imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "    cv2.imwrite('matches.jpg',imMatches)\n",
    "    \n",
    "    # Extract location of good matches\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "    \n",
    "    for i, match in enumerate(matches):\n",
    "        points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "        points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "        \n",
    "    # Find homography\n",
    "    h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    "    \n",
    "    # Use homography\n",
    "    height, width, channels = im2.shape\n",
    "    im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "    \n",
    "    return im1Reg, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "refFilename = r\"C:\\Users\\pedro\\Python\\form.jpg\"\n",
    "imReference = cv2.imread(refFilename, cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e3457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Read reference image\n",
    "    refFilename = r\"C:\\Users\\pedro\\Python\\form.jpg\"\n",
    "    print(\"Reading reference image : \", refFilename)\n",
    "    imReference = cv2.imread(refFilename, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    # Read image to be aligned\n",
    "    imFilename = r\"C:\\Users\\pedro\\Python\\scanned_form.jpg\"\n",
    "    print(\"Reading image to align : \", imFilename);\n",
    "    im = cv2.imread(imFilename, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    print(\"Aligning images ...\")\n",
    "    # Registered image will be resotred in imReg.\n",
    "    # The estimated homography will be stored in h.\n",
    "    imReg, h = alignImages(im, imReference)\n",
    "    \n",
    "    # Write aligned image to disk.\n",
    "    outFilename = \"aligned.jpg\"\n",
    "    print(\"Saving aligned image : \", outFilename);\n",
    "    cv2.imwrite(outFilename, imReg)\n",
    "    \n",
    "    # Print estimated homography\n",
    "    print(\"Estimated homography : \\n\",  h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d71899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2       # OpenCV\n",
    "\n",
    "# 1. Load the original image\n",
    "img = cv2.imread('form.jpg')\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 2. Create BRISK algorithm\n",
    "# OpenCV default threshold = 30, octaves = 3\n",
    "# Using 4 octaves as cited as typical value by the original paper by Leutenegger et al.\n",
    "# Using 70 as detection threshold similar to real-world example of this paper\n",
    "brisk = cv2.BRISK_create(70, 4)\n",
    "\n",
    "# 3. Combined call to let the BRISK implementation detect keypoints\n",
    "# as well as calculate the descriptors, based on the grayscale image.\n",
    "# These are returned in two arrays.\n",
    "(kps, descs) = brisk.detectAndCompute(gray, None)\n",
    "\n",
    "# 4. Print the number of keypoints and descriptors found\n",
    "print(\"# keypoints: {}, descriptors: {}\".format(len(kps), descs.shape))\n",
    "# To verify: how many bits are contained in a feature descriptor?\n",
    "# Should be 64 * 8 = 512 bits according to the algorithm paper.\n",
    "print(len(descs[1]) * 8)\n",
    "\n",
    "# 5. Use the generic drawKeypoints method from OpenCV to draw the \n",
    "# calculated keypoints into the original image.\n",
    "# The flag for rich keypoints also draws circles to indicate\n",
    "# direction and scale of the keypoints.\n",
    "imgBrisk = cv2.drawKeypoints(gray, kps, img, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# 6. Finally, write the resulting image to the disk\n",
    "cv2.imwrite('brisk_keypoints.jpg', imgBrisk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecb1a563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_corrected_img(img1, img2,MIN_MATCHES = 50):\n",
    "    MIN_MATCHES = 50\n",
    "\n",
    "    orb = cv2.ORB_create(nfeatures=500)\n",
    "    kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "    kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "    index_params = dict(algorithm=6,\n",
    "                        table_number=6,\n",
    "                        key_size=12,\n",
    "                        multi_probe_level=2)\n",
    "    search_params = {}\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    # As per Lowe's ratio test to filter good matches\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    if len(good_matches) > MIN_MATCHES:\n",
    "        src_points = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_points = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        m, mask = cv2.findHomography(src_points, dst_points, cv2.RANSAC, 5.0)\n",
    "        corrected_img = cv2.warpPerspective(img1, m, (img2.shape[1], img2.shape[0]))\n",
    "        print(True)\n",
    "        return corrected_img\n",
    "    print(False)\n",
    "    return img2\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    im1 = cv2.imread('form2.jpg')\n",
    "    im2 = cv2.imread('scanned_form2.jpg')\n",
    "\n",
    "    img = get_corrected_img(im1,im2,100)\n",
    "    cv2.imwrite('corimg.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f42a91b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "\n",
    "temp1 = cv2.imread('form2.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "temp2 = cv2.imread('scanned_form2.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "temp11 = cv2.medianBlur(temp1,13)\n",
    "temp22 = cv2.medianBlur(temp2,13)\n",
    "\n",
    "img1 = cv2.Canny(temp11,100,70)\n",
    "img2 = cv2.Canny(temp22,100,70)\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "kp1, des1 = orb.detectAndCompute(temp11, None)\n",
    "kp2, des2 = orb.detectAndCompute(temp22, None)\n",
    "\n",
    "# matcher takes normType, which is set to cv2.NORM_L2 for SIFT and SURF, cv2.NORM_HAMMING for ORB, FAST and BRIEF\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1, des2)\n",
    "matches = sorted(matches, key=lambda x: x.distance)# draw first 50 matches\n",
    "match_img = cv2.drawMatches(temp11, kp1, temp22, kp2, matches[:100], None)\n",
    "\n",
    "# Downsize and maintain aspect ratio\n",
    "showimg = imutils.resize(match_img, height=600)\n",
    "\n",
    "cv2.imshow('corrected_form.jpg',showimg)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20acd35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "can = cv2.Canny(img2,200,100)\n",
    "\n",
    "cv2.imshow('can',can)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "tmp1 = cv.imread('mask.jpg',cv.IMREAD_GRAYSCALE)          # queryImage\n",
    "tmp2 = cv.imread('scanned_form.jpg',cv.IMREAD_GRAYSCALE) # trainImage\n",
    "\n",
    "ret,img1 = cv2.threshold(tmp1,100,255,cv2.THRESH_BINARY)\n",
    "ret,img2 = cv2.threshold(tmp2,100,255,cv2.THRESH_BINARY)\n",
    "\n",
    "# Initiate SIFT detector\n",
    "sift = cv.SIFT_create()\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "flann = cv.FlannBasedMatcher(index_params,search_params)\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "# Need to draw only good matches, so create a mask\n",
    "matchesMask = [[0,0] for i in range(len(matches))]\n",
    "# ratio test as per Lowe's paper\n",
    "for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < 0.6*n.distance:\n",
    "        matchesMask[i]=[1,0]\n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    "                   singlePointColor = (255,0,0),\n",
    "                   matchesMask = matchesMask,\n",
    "                   flags = cv.DrawMatchesFlags_DEFAULT)\n",
    "img3 = cv.drawMatchesKnn(img1,kp1,img2,kp2,matches,None,**draw_params)\n",
    "\n",
    "# Downsize and maintain aspect ratio\n",
    "showimg = imutils.resize(img3, height=600)\n",
    "\n",
    "cv2.imshow('corrected_form.jpg',showimg)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a06be143",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv.imread('scanned_form.jpg')          # queryImage# convert the image to grayscale and blur it slightly\n",
    "gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (11, 11), 0)\n",
    "ret,thresh1 = cv2.threshold(gray,100,255,cv2.THRESH_BINARY)\n",
    "cv2.imshow('can',thresh1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a80b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') \n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml') \n",
    "\n",
    "\n",
    "faces = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') \n",
    "eyes = cv2.CascadeClassifier('haarcascade_eye.xml') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dd5daaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(image):\n",
    "    original_image = image\n",
    "    processed_img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    processed_img =  cv2.Canny(processed_img, threshold1 = 200, threshold2=300)\n",
    "    return processed_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "75d96640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 37  44  44  44]\n",
      " [112  68  41  41]]\n",
      "40.5 44.0\n",
      "76.5 54.5\n"
     ]
    }
   ],
   "source": [
    "frame = cv.imread('scanned_form.jpg')\n",
    "gray_frame=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "face=faces.detectMultiScale(frame,1.3,5)\n",
    "\n",
    "# now getting into the face and its position\n",
    "for (x,y,w,h) in face:\n",
    "    # drawing the rectangle on the face\n",
    "    cv2.rectangle(frame,(x,y),(x+w,y+h),(0,0,255),thickness=4)\n",
    "\n",
    "    # now the eyes are on the face\n",
    "    # so we have to make the face frame gray\n",
    "    gray_face=gray_frame[y:y+h,x:x+w]\n",
    "\n",
    "    # make the color face also\n",
    "    color_face=frame[y:y+h,x:x+w]\n",
    "\n",
    "    # check the eyes on this face\n",
    "    eye=eyes.detectMultiScale(gray_face,1.3,5)\n",
    "    print(eye)\n",
    "    # get into the eyes with its position\n",
    "    for (a,b,c,d) in eye:\n",
    "        # we have to draw the rectangle on the\n",
    "        # coloured face\n",
    "        print(a+((c-a)/2),b+((d-b)/2))\n",
    "        cv2.rectangle(color_face,(a,b),(a+c,b+d),(0,255,0),thickness=4)\n",
    "\n",
    "    # show the frame\n",
    "# Downsize and maintain aspect ratio\n",
    "showimg = imutils.resize(frame, height=500)\n",
    "\n",
    "cv2.imshow('corrected_form.jpg',showimg)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a605208",
   "metadata": {},
   "outputs": [],
   "source": [
    "a+((c-a)/2),b+((d-b)/2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
